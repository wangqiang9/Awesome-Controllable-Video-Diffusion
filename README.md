# Awesome-Controllable-Video-Diffusion
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/hee9joon/Awesome-Diffusion-Models) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

Awesome Controllable Video Generation with Diffusion Models.

## Table of Contents
- [Pose Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#pose-control)
- [Audio Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#audio-control)
- [Universal Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#universal-control)
- [Camera Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#camera-control)
- [Trajectory Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#trajectory-control)
- [Subject Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#subject-control)

## Pose Control
Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos

[ğŸ“„ Paper](https://arxiv.org/abs/2304.01186) | [ğŸŒ Project Page](https://follow-your-pose.github.io/) [ğŸ’» Code](https://github.com/mayuelala/FollowYourPose)

Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation

[ğŸ“„ Paper](https://arxiv.org/pdf/2311.17117.pdf) | [ğŸŒ Project Page](https://humanaigc.github.io/animate-anyone/)

DreaMoving: A Human Video Generation Framework based on Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2312.05107) | [ğŸŒ Project Page](https://dreamoving.github.io/dreamoving/) [ğŸ’» Code](https://github.com/dreamoving/dreamoving-project)

MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion

[ğŸ“„ Paper](https://arxiv.org/abs/2311.12052) | [ğŸŒ Project Page](https://boese0601.github.io/magicdance/) [ğŸ’» Code](https://github.com/Boese0601/MagicDance)

MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model

[ğŸ“„ Paper](https://arxiv.org/abs/2406.01188) | [ğŸŒ Project Page](https://showlab.github.io/magicanimate/) [ğŸ’» Code](https://github.com/magic-research/magic-animate)

Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance

[ğŸ“„ Paper](https://arxiv.org/pdf/2403.14781) | [ğŸŒ Project Page](https://fudan-generative-vision.github.io/champ/#/) [ğŸ’» Code](https://github.com/fudan-generative-vision/champ)

Magic-Me: Identity-Specific Video Customized Diffusion

[ğŸ“„ Paper](https://arxiv.org/abs/2402.09368) | [ğŸŒ Project Page](https://magic-me-webpage.github.io/) [ğŸ’» Code](https://github.com/Zhen-Dong/Magic-Me)

DisCo: Disentangled Control for Referring Human Dance Generation in Real World

[ğŸ“„ Paper](https://arxiv.org/abs/2307.00040) | [ğŸŒ Project Page](https://disco-dance.github.io/) [ğŸ’» Code](https://github.com/Wangt-CN/DisCo)

Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer

[ğŸ“„ Paper](https://arxiv.org/abs/2405.17405) | [ğŸŒ Project Page](https://human4dit.github.io/)

MimicMotion : High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance

[ğŸ“„ Paper](https://arxiv.org/abs/2406.19680) | [ğŸŒ Project Page](https://tencent.github.io/MimicMotion/) [ğŸ’» Code](https://github.com/tencent/MimicMotion)

Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control

[ğŸ“„ Paper](https://arxiv.org/abs/2406.03035) | [ğŸŒ Project Page](https://follow-your-pose-v2.github.io/)

HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation

[ğŸ“„ Paper](https://arxiv.org/abs/2407.17438) | [ğŸŒ Project Page](https://humanvid.github.io/) [ğŸ’» Code](https://github.com/zhenzhiwang/HumanVid)

MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation.

[ğŸ’» Code](https://github.com/TMElyralab/MusePose)

MDM: Human Motion Diffusion Model

[ğŸ“„ Paper](https://arxiv.org/abs/2209.14916) | [ğŸŒ Project Page](https://guytevet.github.io/mdm-page/) [ğŸ’» Code](https://github.com/GuyTevet/motion-diffusion-model)

## Audio Control

Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model

[ğŸ“„ Paper](https://arxiv.org/pdf/2404.01862) | [ğŸŒ Project Page](https://thuhcsi.github.io/S2G-MDDiffusion/) [ğŸ’» Code](https://github.com/thuhcsi/S2G-MDDiffusion)

Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation

[ğŸ“„ Paper](https://arxiv.org/abs/2309.16429) | [ğŸŒ Project Page](https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/) [ğŸ’» Code](https://github.com/guyyariv/TempoTokens)

MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2212.09478) | [ğŸ’» Code](https://github.com/researchmm/MM-Diffusion)

Speech Driven Video Editing via an Audio-Conditioned Diffusion Model

[ğŸ“„ Paper](https://arxiv.org/abs/2301.04474) | [ğŸŒ Project Page](https://danbigioi.github.io/DiffusionVideoEditing/) [ğŸ’» Code](https://github.com/DanBigioi/DiffusionVideoEditing)

Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation

[ğŸ“„ Paper](https://arxiv.org/pdf/2406.08801) | [ğŸŒ Project Page](https://fudan-generative-vision.github.io/hallo/#/) [ğŸ’» Code](https://github.com/fudan-generative-vision/hallo)

Listen, denoise, action! Audio-driven motion synthesis with diffusion models

[ğŸ“„ Paper](https://arxiv.org/abs/2211.09707) | [ğŸŒ Project Page](https://www.speech.kth.se/research/listen-denoise-action/) [ğŸ’» Code](https://github.com/simonalexanderson/ListenDenoiseAction/)

## Universal Control
ControlNeXt: Powerful and Efficient Control for Image and Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2408.06070) | [ğŸŒ Project Page](https://pbihao.github.io/projects/controlnext/index.html) [ğŸ’» Code](https://github.com/dvlab-research/ControlNeXt)

TrackGo: A Flexible and Efficient Method for Controllable Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2408.11475) | [ğŸŒ Project Page](https://zhtjtcz.github.io/TrackGo-Page/#) [ğŸ’» Code](https://zhtjtcz.github.io/TrackGo-Page/#)

VideoComposer: Compositional Video Synthesis with Motion Controllability

[ğŸ“„ Paper](https://arxiv.org/abs/2306.02018) | [ğŸŒ Project Page](https://videocomposer.github.io/) [ğŸ’» Code](https://github.com/damo-vilab/videocomposer)

Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance

[ğŸ“„ Paper](https://arxiv.org/abs/2306.00943) | [ğŸŒ Project Page](https://doubiiu.github.io/projects/Make-Your-Video/) [ğŸ’» Code](https://github.com/VideoCrafter/Make-Your-Video)

UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control

[ğŸ“„ Paper](https://arxiv.org/pdf/2403.02332.pdf) | [ğŸŒ Project Page](https://unified-attention-control.github.io/) [ğŸ’» Code](https://github.com/XuweiyiChen/UniCtrl)

SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2311.16933) | [ğŸŒ Project Page](https://guoyww.github.io/projects/SparseCtrl/) [ğŸ’» Code](https://github.com/guoyww/AnimateDiff#202312-animatediff-v3-and-sparsectrl)

## Camera Control

Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion

[ğŸ“„ Paper](https://arxiv.org/abs/2402.03162) | [ğŸŒ Project Page](https://direct-a-video.github.io/) [ğŸ’» Code](https://github.com/ysy31415/direct_a_video)

MotionCtrl: A Unified and Flexible Motion Controller for Video Generation

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.03641.pdf) | [ğŸŒ Project Page](https://wzhouxiff.github.io/projects/MotionCtrl/) [ğŸ’» Code](https://github.com/TencentARC/MotionCtrl)

CameraCtrl: Enabling Camera Control for Text-to-Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2404.02101) | [ğŸŒ Project Page](https://hehao13.github.io/projects-CameraCtrl/) [ğŸ’» Code](https://github.com/hehao13/CameraCtrl)

VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control

[ğŸ“„ Paper](https://arxiv.org/abs/2407.12781) | [ğŸŒ Project Page](https://snap-research.github.io/vd3d/)

Controlling Space and Time with Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/pdf/2407.07860) | [ğŸŒ Project Page](https://4d-diffusion.github.io/)

CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2406.02509) | [ğŸŒ Project Page](https://ir1d.github.io/CamCo/)

Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control

[ğŸ“„ Paper](https://arxiv.org/pdf/2405.17414) | [ğŸŒ Project Page](https://collaborativevideodiffusion.github.io/)

HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation

[ğŸ“„ Paper](https://arxiv.org/abs/2407.17438) | [ğŸŒ Project Page](https://humanvid.github.io/) [ğŸ’» Code](https://github.com/zhenzhiwang/HumanVid)

Training-free Camera Control for Video Generation

[ğŸ“„ Paper](https://arxiv.org/pdf/2406.10126) | [ğŸŒ Project Page](https://lifedecoder.github.io/CamTrol/)

Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text

[ğŸ“„ Paper](https://arxiv.org/pdf/2406.17601) | [ğŸŒ Project Page](https://imlixinyang.github.io/director3d-page/) [ğŸ’» Code](https://github.com/imlixinyang/director3d)

MotionBooth: Motion-Aware Customized Text-to-Video Generation

[ğŸ“„ Paper](http://arxiv.org/abs/2406.17758v1) | [ğŸ’» Code](https://github.com/jianzongwu/MotionBooth)


## Trajectory Control

FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2406.16863) | [ğŸŒ Project Page](http://haonanqiu.com/projects/FreeTraj.html) [ğŸ’» Code](https://github.com/arthur-qiu/FreeTraj)

TrailBlazer: Trajectory Control for Diffusion-Based Video Generation

[ğŸ“„ Paper](http://arxiv.org/abs/2401.00896) | [ğŸŒ Project Page](https://hohonu-vicml.github.io/Trailblazer.Page/) [ğŸ’» Code](https://github.com/hohonu-vicml/Trailblazer)

DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory

[ğŸ“„ Paper](https://www.microsoft.com/en-us/research/publication/dragnuwa-fine-grained-control-in-video-generation-by-integrating-text-image-and-trajectory/bibtex/) | [ğŸŒ Project Page](https://www.microsoft.com/en-us/research/project/dragnuwa/) [ğŸ’» Code](https://github.com/ProjectNUWA/DragNUWA)

Tora: Trajectory-oriented Diffusion Transformer for Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2407.21705) | [ğŸŒ Project Page](https://ali-videoai.github.io/tora_video/)

Controllable Longer Image Animation with Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2405.17306) | [ğŸŒ Project Page](https://wangqiang9.github.io/Controllable.github.io/)

MotionCtrl: A Unified and Flexible Motion Controller for Video Generation

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.03641.pdf) | [ğŸŒ Project Page](https://wzhouxiff.github.io/projects/MotionCtrl/) [ğŸ’» Code](https://github.com/TencentARC/MotionCtrl)

MotionBooth: Motion-Aware Customized Text-to-Video Generation

[ğŸ“„ Paper](http://arxiv.org/abs/2406.17758v1) | [ğŸ’» Code](https://github.com/jianzongwu/MotionBooth)

Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion

[ğŸ“„ Paper](https://arxiv.org/abs/2402.03162) | [ğŸŒ Project Page](https://direct-a-video.github.io/) [ğŸ’» Code](https://github.com/ysy31415/direct_a_video)

Generative Image Dynamics

[ğŸ“„ Paper](https://arxiv.org/abs/2309.07906) | [ğŸŒ Project Page](https://generative-dynamics.github.io/)

## Subject Control

Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion

[ğŸ“„ Paper](https://arxiv.org/abs/2402.03162) | [ğŸŒ Project Page](https://direct-a-video.github.io/) [ğŸ’» Code](https://github.com/ysy31415/direct_a_video)

ActAnywhere: Subject-Aware Video Background Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2401.10822) | [ğŸŒ Project Page](https://actanywhere.github.io/)


