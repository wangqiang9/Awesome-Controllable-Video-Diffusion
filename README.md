# Awesome-Controllable-Video-Diffusion
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/hee9joon/Awesome-Diffusion-Models) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

Awesome Controllable Video Generation with Diffusion Models.

## Table of Contents
- [Pose Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#pose-control)
- [Audio Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#audio-control)
- [Universal Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#universal-control)
- [Camera Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#camera-control)
- [Trajectory Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#trajectory-control)
- [Subject Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion#subject-control)
- [Area Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion/blob/main/README.md#area-control)
- [Video Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion/blob/main/README.md#video-control)
- [Brain Control](https://github.com/wangqiang9/Awesome-Controllable-Video-Diffusion/blob/main/README.md#brain-control)

## Pose Control

DynamicPose: A robust image-to-video framework for portrait animation driven by pose sequences

 [ğŸ’» Code](https://github.com/dynamic-X-LAB/DynamicPose)

Alignment is All You Need: A Training-free Augmentation Strategy for Pose-guided Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2408.16506)

Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos

[ğŸ“„ Paper](https://arxiv.org/abs/2304.01186) | [ğŸŒ Project Page](https://follow-your-pose.github.io/) | [ğŸ’» Code](https://github.com/mayuelala/FollowYourPose)

Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation

[ğŸ“„ Paper](https://arxiv.org/pdf/2311.17117.pdf) | [ğŸŒ Project Page](https://humanaigc.github.io/animate-anyone/)

DreaMoving: A Human Video Generation Framework based on Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2312.05107) | [ğŸŒ Project Page](https://dreamoving.github.io/dreamoving/) | [ğŸ’» Code](https://github.com/dreamoving/dreamoving-project)

MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion

[ğŸ“„ Paper](https://arxiv.org/abs/2311.12052) | [ğŸŒ Project Page](https://boese0601.github.io/magicdance/) | [ğŸ’» Code](https://github.com/Boese0601/MagicDance)

MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model

[ğŸ“„ Paper](https://arxiv.org/abs/2406.01188) | [ğŸŒ Project Page](https://showlab.github.io/magicanimate/) | [ğŸ’» Code](https://github.com/magic-research/magic-animate)

Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance

[ğŸ“„ Paper](https://arxiv.org/pdf/2403.14781) | [ğŸŒ Project Page](https://fudan-generative-vision.github.io/champ/#/) | [ğŸ’» Code](https://github.com/fudan-generative-vision/champ)

Magic-Me: Identity-Specific Video Customized Diffusion

[ğŸ“„ Paper](https://arxiv.org/abs/2402.09368) | [ğŸŒ Project Page](https://magic-me-webpage.github.io/) | [ğŸ’» Code](https://github.com/Zhen-Dong/Magic-Me)

DisCo: Disentangled Control for Referring Human Dance Generation in Real World

[ğŸ“„ Paper](https://arxiv.org/abs/2307.00040) | [ğŸŒ Project Page](https://disco-dance.github.io/) | [ğŸ’» Code](https://github.com/Wangt-CN/DisCo)

Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer

[ğŸ“„ Paper](https://arxiv.org/abs/2405.17405) | [ğŸŒ Project Page](https://human4dit.github.io/)

MimicMotion : High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance

[ğŸ“„ Paper](https://arxiv.org/abs/2406.19680) | [ğŸŒ Project Page](https://tencent.github.io/MimicMotion/) | [ğŸ’» Code](https://github.com/tencent/MimicMotion)

Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control

[ğŸ“„ Paper](https://arxiv.org/abs/2406.03035) | [ğŸŒ Project Page](https://follow-your-pose-v2.github.io/)

HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation

[ğŸ“„ Paper](https://arxiv.org/abs/2407.17438) | [ğŸŒ Project Page](https://humanvid.github.io/) | [ğŸ’» Code](https://github.com/zhenzhiwang/HumanVid)

MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation.

[ğŸ’» Code](https://github.com/TMElyralab/MusePose)

MDM: Human Motion Diffusion Model

[ğŸ“„ Paper](https://arxiv.org/abs/2209.14916) | [ğŸŒ Project Page](https://guytevet.github.io/mdm-page/)  | [ğŸ’» Code](https://github.com/GuyTevet/motion-diffusion-model)

## Audio Control

Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model

[ğŸ“„ Paper](https://arxiv.org/pdf/2404.01862) | [ğŸŒ Project Page](https://thuhcsi.github.io/S2G-MDDiffusion/) | [ğŸ’» Code](https://github.com/thuhcsi/S2G-MDDiffusion)

Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation

[ğŸ“„ Paper](https://arxiv.org/abs/2309.16429) | [ğŸŒ Project Page](https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/) | [ğŸ’» Code](https://github.com/guyyariv/TempoTokens)

MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2212.09478) | [ğŸ’» Code](https://github.com/researchmm/MM-Diffusion)

Speech Driven Video Editing via an Audio-Conditioned Diffusion Model

[ğŸ“„ Paper](https://arxiv.org/abs/2301.04474) | [ğŸŒ Project Page](https://danbigioi.github.io/DiffusionVideoEditing/) | [ğŸ’» Code](https://github.com/DanBigioi/DiffusionVideoEditing)

Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation

[ğŸ“„ Paper](https://arxiv.org/pdf/2406.08801) | [ğŸŒ Project Page](https://fudan-generative-vision.github.io/hallo/#/) | [ğŸ’» Code](https://github.com/fudan-generative-vision/hallo)

Listen, denoise, action! Audio-driven motion synthesis with diffusion models

[ğŸ“„ Paper](https://arxiv.org/abs/2211.09707) | [ğŸŒ Project Page](https://www.speech.kth.se/research/listen-denoise-action/) | [ğŸ’» Code](https://github.com/simonalexanderson/ListenDenoiseAction/)

CoDi: Any-to-Any Generation via Composable Diffusion

[ğŸ“„ Paper](http://arxiv.org/abs/2305.11846) | [ğŸŒ Project Page](https://codi-gen.github.io/) | [ğŸ’» Code](https://github.com/microsoft/i-Code/tree/main/i-Code-V3)

Generative Disco: Text-to-Video Generation for Music Visualization

[ğŸ“„ Paper](https://arxiv.org/abs/2304.08551)

AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion

[ğŸ“„ Paper](https://arxiv.org/abs/2305.04001)

EMO: Emote Portrait Alive Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions

[ğŸ“„ Paper](https://arxiv.org/abs/2402.17485) | [ğŸŒ Project Page](https://humanaigc.github.io/emote-portrait-alive/) | [ğŸ’» Code](https://github.com/HumanAIGC/EMO)

Context-aware Talking Face Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2402.18092)

## Universal Control
ControlNeXt: Powerful and Efficient Control for Image and Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2408.06070) | [ğŸŒ Project Page](https://pbihao.github.io/projects/controlnext/index.html) | [ğŸ’» Code](https://github.com/dvlab-research/ControlNeXt)

Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2408.06070) | [ğŸŒ Project Page](https://controlavideo.github.io/) | [ğŸ’» Code](https://github.com/Weifeng-Chen/control-a-video)

ControlVideo: Training-free Controllable Text-to-Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2305.13077) | [ğŸ’» Code](https://github.com/YBYBZhang/ControlVideo)

TrackGo: A Flexible and Efficient Method for Controllable Video Generation

[ğŸ“„ Paper](https://controlavideo.github.io/#paper) | [ğŸŒ Project Page](https://zhtjtcz.github.io/TrackGo-Page/#) | [ğŸ’» Code](https://zhtjtcz.github.io/TrackGo-Page/#)

VideoComposer: Compositional Video Synthesis with Motion Controllability

[ğŸ“„ Paper](https://arxiv.org/abs/2306.02018) | [ğŸŒ Project Page](https://videocomposer.github.io/) | [ğŸ’» Code](https://github.com/damo-vilab/videocomposer)

Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance

[ğŸ“„ Paper](https://arxiv.org/abs/2306.00943) | [ğŸŒ Project Page](https://doubiiu.github.io/projects/Make-Your-Video/) | [ğŸ’» Code](https://github.com/VideoCrafter/Make-Your-Video)

UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control

[ğŸ“„ Paper](https://arxiv.org/pdf/2403.02332.pdf) | [ğŸŒ Project Page](https://unified-attention-control.github.io/) | [ğŸ’» Code](https://github.com/XuweiyiChen/UniCtrl)

SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2311.16933) | [ğŸŒ Project Page](https://guoyww.github.io/projects/SparseCtrl/) | [ğŸ’» Code](https://github.com/guoyww/AnimateDiff#202312-animatediff-v3-and-sparsectrl)

VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet

[ğŸ“„ Paper](https://arxiv.org/abs/2307.14073) | [ğŸŒ Project Page](https://vcg-aigc.github.io/) | [ğŸ’» Code](https://github.com/ZhihaoHu/VideoControlNet)

Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models  

[ğŸ“„ Paper](https://arxiv.org/abs/2407.15642) | [ğŸŒ Project Page](https://maxin-cn.github.io/cinemo_project/) | [ğŸ’» Code](https://github.com/maxin-cn/Cinemo)

## Camera Control

CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion

[ğŸ“„ Paper](https://arxiv.org/html/2408.17424v1)

CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers

[ğŸ“„ Paper](https://arxiv.org/abs/2405.13195)

Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion

[ğŸ“„ Paper](https://arxiv.org/abs/2402.03162) | [ğŸŒ Project Page](https://direct-a-video.github.io/) | [ğŸ’» Code](https://github.com/ysy31415/direct_a_video)

MotionCtrl: A Unified and Flexible Motion Controller for Video Generation

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.03641.pdf) | [ğŸŒ Project Page](https://wzhouxiff.github.io/projects/MotionCtrl/) | [ğŸ’» Code](https://github.com/TencentARC/MotionCtrl)

CameraCtrl: Enabling Camera Control for Text-to-Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2404.02101) | [ğŸŒ Project Page](https://hehao13.github.io/projects-CameraCtrl/) | [ğŸ’» Code](https://github.com/hehao13/CameraCtrl)

VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control

[ğŸ“„ Paper](https://arxiv.org/abs/2407.12781) | [ğŸŒ Project Page](https://snap-research.github.io/vd3d/)

Controlling Space and Time with Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/pdf/2407.07860) | [ğŸŒ Project Page](https://4d-diffusion.github.io/)

CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2406.02509) | [ğŸŒ Project Page](https://ir1d.github.io/CamCo/)

Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control

[ğŸ“„ Paper](https://arxiv.org/pdf/2405.17414) | [ğŸŒ Project Page](https://collaborativevideodiffusion.github.io/)

HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation

[ğŸ“„ Paper](https://arxiv.org/abs/2407.17438) | [ğŸŒ Project Page](https://humanvid.github.io/) | [ğŸ’» Code](https://github.com/zhenzhiwang/HumanVid)

Training-free Camera Control for Video Generation

[ğŸ“„ Paper](https://arxiv.org/pdf/2406.10126) | [ğŸŒ Project Page](https://lifedecoder.github.io/CamTrol/)

Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text

[ğŸ“„ Paper](https://arxiv.org/pdf/2406.17601) | [ğŸŒ Project Page](https://imlixinyang.github.io/director3d-page/) | [ğŸ’» Code](https://github.com/imlixinyang/director3d)

MotionBooth: Motion-Aware Customized Text-to-Video Generation

[ğŸ“„ Paper](http://arxiv.org/abs/2406.17758v1) | [ğŸ’» Code](https://github.com/jianzongwu/MotionBooth)

DiffDreamer: Towards Consistent Unsupervised Single-view Scene Extrapolation with Conditional Diffusion Models

[ğŸ“„ Paper](https://primecai.github.io/static/pdfs/diffdreamer.pdf) | [ğŸŒ Project Page]([https://imlixinyang.github.io/director3d-page/](https://primecai.github.io/diffdreamer))

## Trajectory Control

FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2406.16863) | [ğŸŒ Project Page](http://haonanqiu.com/projects/FreeTraj.html) | [ğŸ’» Code](https://github.com/arthur-qiu/FreeTraj)

TrailBlazer: Trajectory Control for Diffusion-Based Video Generation

[ğŸ“„ Paper](http://arxiv.org/abs/2401.00896) | [ğŸŒ Project Page](https://hohonu-vicml.github.io/Trailblazer.Page/) | [ğŸ’» Code](https://github.com/hohonu-vicml/Trailblazer)

DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory

[ğŸ“„ Paper](https://www.microsoft.com/en-us/research/publication/dragnuwa-fine-grained-control-in-video-generation-by-integrating-text-image-and-trajectory/bibtex/) | [ğŸŒ Project Page](https://www.microsoft.com/en-us/research/project/dragnuwa/) | [ğŸ’» Code](https://github.com/ProjectNUWA/DragNUWA)

Tora: Trajectory-oriented Diffusion Transformer for Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2407.21705) | [ğŸŒ Project Page](https://ali-videoai.github.io/tora_video/)

Controllable Longer Image Animation with Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/abs/2405.17306) | [ğŸŒ Project Page](https://wangqiang9.github.io/Controllable.github.io/)

MotionCtrl: A Unified and Flexible Motion Controller for Video Generation

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.03641.pdf) | [ğŸŒ Project Page](https://wzhouxiff.github.io/projects/MotionCtrl/) | [ğŸ’» Code](https://github.com/TencentARC/MotionCtrl)

MotionBooth: Motion-Aware Customized Text-to-Video Generation

[ğŸ“„ Paper](http://arxiv.org/abs/2406.17758v1) | [ğŸ’» Code](https://github.com/jianzongwu/MotionBooth)

Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics

[ğŸ“„ Paper](https://arxiv.org/pdf/2408.04631) | [ğŸŒ Project Page](https://vgg-puppetmaster.github.io/) | [ğŸ’» Code](https://github.com/RuiningLi/puppet-master)

Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion

[ğŸ“„ Paper](https://arxiv.org/abs/2402.03162) | [ğŸŒ Project Page](https://direct-a-video.github.io/) | [ğŸ’» Code](https://github.com/ysy31415/direct_a_video)

Generative Image Dynamics

[ğŸ“„ Paper](https://arxiv.org/abs/2309.07906) | [ğŸŒ Project Page](https://generative-dynamics.github.io/)

Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2401.10150)

Video Diffusion Models are Training-free Motion Interpreter and Controlle

[ğŸ“„ Paper](https://arxiv.org/abs/2405.14864) | [ğŸŒ Project Page](https://xizaoqu.github.io/moft/)

## Subject Control

Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion

[ğŸ“„ Paper](https://arxiv.org/abs/2402.03162) | [ğŸŒ Project Page](https://direct-a-video.github.io/) | [ğŸ’» Code](https://github.com/ysy31415/direct_a_video)

ActAnywhere: Subject-Aware Video Background Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2401.10822) | [ğŸŒ Project Page](https://actanywhere.github.io/)

MotionBooth: Motion-Aware Customized Text-to-Video Generation

[ğŸ“„ Paper](http://arxiv.org/abs/2406.17758v1) | [ğŸ’» Code](https://github.com/jianzongwu/MotionBooth)

Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2307.06940) | [ğŸ’» Code](https://github.com/AILab-CVC/Animate-A-Story)

One-Shot Learning Meets Depth Diffusion in Multi-Object Videos

[ğŸ“„ Paper](https://arxiv.org/abs/2408.16704)

## Area Control

Boximator: Generating Rich and Controllable Motions for Video Synthesis

[ğŸ“„ Paper](https://arxiv.org/abs/2402.01566) | [ğŸŒ Project Page](https://boximator.github.io/)

Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts

[ğŸ“„ Paper](https://arxiv.org/abs/2403.08268) | [ğŸŒ Project Page](https://follow-your-click.github.io/) | [ğŸ’» Code](https://github.com/mayuelala/FollowYourClick)

AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance

[ğŸ“„ Paper](https://arxiv.org/pdf/2311.12886.pdf) | [ğŸŒ Project Page](https://animationai.github.io/AnimateAnything/) | [ğŸ’» Code](https://github.com/alibaba/animate-anything)

Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling

[ğŸ“„ Paper](https://arxiv.org/abs/2401.15977) | [ğŸŒ Project Page](https://xiaoyushi97.github.io/Motion-I2V/)

Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion

[ğŸ“„ Paper](https://arxiv.org/abs/2407.13759) | [ğŸŒ Project Page](https://boyangdeng.com/streetscapes/)

## Video Control

Customizing Motion in Text-to-Video Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.04966.pdf) | [ğŸŒ Project Page](https://joaanna.github.io/customizing_motion/)

MotionClone: Training-Free Motion Cloning for Controllable Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2406.05338) | [ğŸŒ Project Page](https://bujiazi.github.io/motionclone.github.io/) | [ğŸ’» Code](https://github.com/Bujiazi/MotionClone)

VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models

[ğŸ“„ Paper](https://video-motion-customization.github.io/static/video-motion-customization(vmc)-arxiv.pdf) | [ğŸŒ Project Page](https://video-motion-customization.github.io/) | [ğŸ’» Code](https://github.com/HyeonHo99/Video-Motion-Customization)

Motion Inversion for Video Customization

[ğŸ“„ Paper](https://arxiv.org/abs/2403.20193) | [ğŸŒ Project Page](https://wileewang.github.io/MotionInversion/) | [ğŸ’» Code](https://github.com/EnVision-Research/MotionInversion)

## Brain Control 

NeuroCine: Decoding Vivid Video Sequences from Human Brain Activties

[ğŸ“„ Paper](https://arxiv.org/abs/2402.01590)

Mind-Video: Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity

[ğŸ“„ Paper](https://arxiv.org/abs/2305.11675) | [ğŸŒ Project Page](https://www.mind-video.com/) | [ğŸ’» Code](https://github.com/jqin4749/MindVideo)


